{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "- Implement impurity measure\n",
    "- Implement a Decision Tree from Scratch\n",
    "  - Find Best Split\n",
    "  - How to recursively build the tree?\n",
    "- Steps on how to build the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity Measures\n",
    "\n",
    "The following are three impurity indices:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Entropy} &= -\\sum_{i=0}^{c-1}  p_i(t) \\log_2 p_i(t) \\\\\n",
    "\\text{Gini index} &= 1 - \\sum_{i=0}^{c-1}  p_i(t)^2 \\\\\n",
    "\\text{Classification error} &= 1 - \\max_i p_i(t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p_i(t)$ is the **relative frequency** of training instances of class $i$ at a node $t$ and $c$ is the number of classes.\n",
    "\n",
    "By convention, we set $0 \\log_2 0 = 0$ in entropy calculations.\n",
    "\n",
    "\n",
    "All three impurity indices equal 0 when all the records at a node belong to the same class.\n",
    "\n",
    "All three impurity indices reach their maximum value when the classes are evenly distributed among the child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for entropy, Gini Index, and Classification Error\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a set of labels.\n",
    "    \"\"\"\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy_value = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    return entropy_value\n",
    "\n",
    "def gini_index(y):\n",
    "    \"\"\"\n",
    "    Calculate the Gini Index of a set of labels.\n",
    "    \"\"\"\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    gini = 1 - np.sum([p ** 2 for p in probabilities])\n",
    "    return gini\n",
    "\n",
    "def classification_error(y):\n",
    "    \"\"\"\n",
    "    Calculate the Classification Error of a set of labels.\n",
    "    \"\"\"\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    error = 1 - np.max(probabilities)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.9709505944546686\n",
      "Gini Index: 0.48\n",
      "Classification Error: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "print(\"Entropy:\", entropy(y))\n",
    "print(\"Gini Index:\", gini_index(y))\n",
    "print(\"Classification Error:\", classification_error(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the best split?\n",
    "Let's take the case of Entropy.\n",
    "\n",
    "To find the best split, we calculate the weighted average entropy after a split\n",
    "\n",
    "The code calculates the **weighted average entropy** after a split. Here's how it works:\n",
    "- `left_y` and `right_y` represent the labels (or target variable) of the left and right subsets of the data after the split.\n",
    "- `len(left_y)` and `len(right_y)` give the number of samples in each subset.\n",
    "- `n` is the total number of samples before the split (i.e., `n = len(left_y) + len(right_y)`).\n",
    "\n",
    "The split criterion (entropy) is calculated as a **weighted sum** of the entropies of the two subsets:\n",
    "\n",
    "$$ \\text{Weighted Entropy} = \\frac{\\text{len(left_y)}}{n} \\times \\text{entropy(left_y)} + \\frac{\\text{len(right_y)}}{n} \\times \\text{entropy(right_y)} $$\n",
    "\n",
    "- The terms `len(left_y) / n` and `len(right_y) / n` are the proportions of samples in each subset.\n",
    "- The entropy of each subset is calculated separately, and then the weighted sum is taken. The weighted sum reflects how \"impure\" the split is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_criterion(self, left_y, right_y):\n",
    "    \"\"\"\n",
    "    Calculate the criterion for splitting.\n",
    "    \"\"\"\n",
    "    n = len(left_y) + len(right_y)\n",
    "    if self.criterion == 'entropy':\n",
    "        return (len(left_y) / n) * entropy(left_y) + (len(right_y) / n) * entropy(right_y)\n",
    "    elif self.criterion == 'gini':\n",
    "        return (len(left_y) / n) * gini_index(left_y) + (len(right_y) / n) * gini_index(right_y)\n",
    "    elif self.criterion == 'classification_error':\n",
    "        return (len(left_y) / n) * classification_error(left_y) + (len(right_y) / n) * classification_error(right_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Class Initialization\n",
    "\n",
    "First, we'll create a `DecisionTree` class and initialize the necessary parameters for our tree such as the splitting criterion, maximum depth, and the tree structure.\n",
    "\n",
    "```python\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion='entropy', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "```\n",
    "\n",
    "## Step 2: Fitting the Model\n",
    "\n",
    "Next, we implement the `fit()` method, which is used to train the decision tree on the given dataset `X` (features) and `y` (target labels). The `fit()` method will call a helper function `_build_tree()` to recursively build the tree.\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    self.tree = self._build_tree(X, y, depth=0)\n",
    "```\n",
    "\n",
    "## Step 3: Recursive Tree Building\n",
    "\n",
    "Now, we implement the logic to build the decision tree recursively. The `_build_tree()` method checks for the base cases (such as when all labels are the same or the maximum depth is reached) and finds the best feature and threshold for splitting the data.\n",
    "\n",
    "```python\n",
    "def _build_tree(self, X, y, depth):\n",
    "    num_samples, num_features = X.shape\n",
    "    if num_samples == 0 or len(set(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "        leaf_value = self._most_common_label(y)\n",
    "        return leaf_value\n",
    "\n",
    "    best_feature, best_threshold = self._best_split(X, y)\n",
    "\n",
    "    left_indices = X[:, best_feature] < best_threshold\n",
    "    right_indices = X[:, best_feature] >= best_threshold\n",
    "\n",
    "    left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "    right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "    return (best_feature, best_threshold, left_subtree, right_subtree)\n",
    "```\n",
    "\n",
    "## Step 4: Finding the Best Split\n",
    "\n",
    "In the next step, we implement `_best_split()` to find the feature and threshold that provide the best split of the data. We'll loop over all features and thresholds to calculate the metric (entropy, gini, etc.) that defines the best split.\n",
    "\n",
    "```python\n",
    "def _best_split(self, X, y):\n",
    "    num_samples, num_features = X.shape\n",
    "    best_metric = float('inf')\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for feature in range(num_features):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            left_indices = X[:, feature] < threshold\n",
    "            right_indices = X[:, feature] >= threshold\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            metric = self._split_criterion(y[left_indices], y[right_indices])\n",
    "            if metric < best_metric:\n",
    "                best_metric = metric\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_feature, best_threshold\n",
    "```\n",
    "\n",
    "## Step 5: Calculating the Split Criterion\n",
    "\n",
    "Here, we define the `_split_criterion()` method to compute the quality of the split using the chosen criterion (entropy, gini, or classification error). Depending on the selected criterion, the appropriate calculation will be performed.\n",
    "\n",
    "```python\n",
    "def _split_criterion(self, left_y, right_y):\n",
    "    n = len(left_y) + len(right_y)\n",
    "    if self.criterion == 'entropy':\n",
    "        return (len(left_y) / n) * entropy(left_y) + (len(right_y) / n) * entropy(right_y)\n",
    "    elif self.criterion == 'gini':\n",
    "        return (len(left_y) / n) * gini_index(left_y) + (len(right_y) / n) * gini_index(right_y)\n",
    "    elif self.criterion == 'classification_error':\n",
    "        return (len(left_y) / n) * classification_error(left_y) + (len(right_y) / n) * classification_error(right_y)\n",
    "```\n",
    "\n",
    "## Step 6: Most Common Label in a Node\n",
    "\n",
    "To handle cases where we need to create a leaf node, we implement `_most_common_label()`. This method returns the most frequent label from the target values.\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "def _most_common_label(self, y):\n",
    "    counter = Counter(y)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    return most_common\n",
    "```\n",
    "\n",
    "## Step 7: Making Predictions\n",
    "\n",
    "Now, we implement the `predict()` method, which allows us to make predictions on new data points. This method will call `_predict()` to traverse the decision tree for each input.\n",
    "\n",
    "```python\n",
    "def predict(self, X):\n",
    "    return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "def _predict(self, inputs):\n",
    "    node = self.tree\n",
    "    while isinstance(node, tuple):\n",
    "        feature, threshold, left_subtree, right_subtree = node\n",
    "        if inputs[feature] < threshold:\n",
    "            node = left_subtree\n",
    "        else:\n",
    "            node = right_subtree\n",
    "    return node\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Decision Tree from scratch\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion='entropy', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        if num_samples == 0 or len(set(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return leaf_value\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = X[:, best_feature] < best_threshold\n",
    "        right_indices = X[:, best_feature] >= best_threshold\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return (best_feature, best_threshold, left_subtree, right_subtree)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_metric = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] < threshold\n",
    "                right_indices = X[:, feature] >= threshold\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                metric = self._split_criterion(y[left_indices], y[right_indices])\n",
    "                if metric < best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _split_criterion(self, left_y, right_y):\n",
    "        \"\"\"\n",
    "        Calculate the criterion for splitting.\n",
    "        \"\"\"\n",
    "        n = len(left_y) + len(right_y)\n",
    "        if self.criterion == 'entropy':\n",
    "            return (len(left_y) / n) * entropy(left_y) + (len(right_y) / n) * entropy(right_y)\n",
    "        elif self.criterion == 'gini':\n",
    "            return (len(left_y) / n) * gini_index(left_y) + (len(right_y) / n) * gini_index(right_y)\n",
    "        elif self.criterion == 'classification_error':\n",
    "            return (len(left_y) / n) * classification_error(left_y) + (len(right_y) / n) * classification_error(right_y)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        \"\"\"\n",
    "        Get the most common label in the dataset.\n",
    "        \"\"\"\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree\n",
    "        while isinstance(node, tuple):\n",
    "            feature, threshold, left_subtree, right_subtree = node\n",
    "            if inputs[feature] < threshold:\n",
    "                node = left_subtree\n",
    "            else:\n",
    "                node = right_subtree\n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a 'sample' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "X = np.array([[2, 3],\n",
    "                [1, 1],\n",
    "                [4, 5],\n",
    "                [3, 2],\n",
    "                [3, 4]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Initialize and fit the decision tree\n",
    "clf = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resurgence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
