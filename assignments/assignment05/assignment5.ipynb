{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"assignment5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering and Gaussian Mixture Models (GMMs)\n",
    "\n",
    "Please complete the following assignment. Run all cells and submit your completed notebook through Gradescope.\n",
    "\n",
    "- Hierarchical clustering is a form of clustering that produces a **set of nested clusters organized in a tree**. It is visualized using a **Dendrogram** (tree-like diagram)\n",
    "\n",
    "#### In this assignment, the aim is to test your knowledge of Hierarchical clustering and GMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "$$\n",
    "\\gamma_{ik} = \\frac{\\pi_k \\cdot \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\cdot \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $\\gamma_{ik}$ is the responsibility of component $k$ for data point $i$.\n",
    "- $\\pi_k$ is the `weight` (or mixing coefficient) for component $k$.\n",
    "- $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ is the Gaussian probability density function for data point $x_i$ given `mean $\\mu_k$` and `covariance $\\Sigma_k$`.\n",
    "- $K$ is the total number of components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 1. In hierarchical clustering, the dendrogram is used to\n",
    "\n",
    "    A) Visualize the data distribution\n",
    "    B) Represent the hierarchy of clusters\n",
    "    C) Compute the distance between clusters\n",
    "    D) Perform dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ans1():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 2. Which of the following is **not** an assumption of Gaussian Mixture Models?\n",
    "    A) Data is generated from a mixture of Gaussian distributions\n",
    "    B) Clusters are independent\n",
    "    C) Each cluster has its own mean and covariance\n",
    "    D) All clusters have equal covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ans2():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 3. Implement hierarchical clustering using SciPy on a generated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "##### 3a. Do the following:\n",
    "- Write a function that takes as input a dataset `X`. Perform a hierarchical clustering according to the following instructions.\n",
    "- Use the **SciPy Cluster Hierarchical linkage** method to and test the following **linkage** methods: ward, complete, average, and single.\n",
    "- Use the **Euclidean** distance metric.\n",
    "- For each linkage method, flatten the hierarchy to $k=2,3,4,5,6,7,8$ clusters and compute the **silhouette score** for each $k$.\n",
    "- Identify which combination of linkage method and cluster number with the `best silhouette score`.\n",
    "- Return a tuple consisting of `(linkage_matrix, linkage_method, k, silhouette score)` for the best linkage method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "def ans3a(X):\n",
    "    # Define linkage methods to test\n",
    "    linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "    best_score = -1\n",
    "    best_method = None\n",
    "    best_k = None\n",
    "    best_Z = None\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "##### 3b. Write a function that plots the dendrogram for your best cluster and confirm that your choice of cluster number is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "def ans3b(Z):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4. Given to you is a generated dataset of 4 3D gaussian clusters with two overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def create_data():\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    cluster_1 = rng.normal(loc=[0, 0, 0], scale=1, size=(100, 3))\n",
    "    cluster_2 = rng.normal(loc=[5, 5, 5], scale=1, size=(100, 3))\n",
    "    cluster_3 = rng.normal(loc=[2, 2, 2], scale=1.5, size=(100, 3))\n",
    "    cluster_4 = rng.normal(loc=[8, 8, 8], scale=1, size=(100, 3))\n",
    "    X = np.vstack((cluster_1, cluster_2, cluster_3, cluster_4))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4a. Use Gaussian Mixture Models (GMM) to identify clusters. Return the `k_values, bics, and silhouette scores`. Please convert all values to `float`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### The Bayesian Information Criterion (BIC) is a statistical measure used to evaluate the goodness of fit of a model while penalizing for complexity.\n",
    "This is how it works:\n",
    "1. BIC assesses how well a model fits the data. **A lower BIC value** indicates a better fit.\n",
    "2. BIC includes a **penalty term** for the number of parameters in the model. (**Discourages overfitting**)\n",
    "3. **BIC = -2 log(L) + k log(n)**      ; L - Likelihood of the model, k - number of parameters, n - number of data points.\n",
    "\n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "- Use `gmm.bic(X)` to find the `bic score` for a given k_value.\n",
    "- For `silhoutte score` computation, use the `labels of the data points` by performing gmm model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### K selection using BIC for GMMs - https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_gmm(X):\n",
    "    bics = []\n",
    "    silhouette_scores = []\n",
    "    k_values = range(2, 10)\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4b. Determine the optimal k by using the `bic` metric to find the best number of clusters. Return the `best_k_bic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimal_k(X):\n",
    "    k_values, bics, silhouette_scores = evaluate_gmm(X)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "4c. Visualize clusters with the best k. Plot a 3D plot and color the points based on the `gmm labels`.\n",
    "Please make sure you `call the function` to display your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_clusters(X):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    best_k = optimal_k(X)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 5. In this question we will implement GMM from scratch. We have filled up most of the code up for you, and require you to fill in the incomplete portions.\n",
    "#### Imagine you’re running a coffee shop and you have data on your customers’ preferences for coffee.\n",
    "\n",
    "##### Each customer likes a different blend of beans, which you can represent as a point in two dimensions: \n",
    "\n",
    "* sweetness (x-axis)\n",
    "* acidity (y-axis). \n",
    "\n",
    "##### Your goal is to `identify the three most popular blends (clusters)` from a pile of customer reviews that provide noisy measurements of these two characteristics.\n",
    "\n",
    "- To do this, we will implement `GMM using the EM algorithm` and cluster the data.\n",
    "1.\tExpectation Step (E-step): The model takes a guess about the `likelihood that each customer belongs to each blend`. At this point, it might not be sure, so it assigns probabilities `(soft assignments)` based on how close the customers’ preferences are to the different blends.\n",
    "2.\tMaximization Step (M-step): The model then `updates its guess about the actual parameters` of the coffee blends -- essentially `adjusting the mean, variance, and proportion of customers for each blend`, based on the soft assignments from the previous step.\n",
    "\n",
    "#### The EM algorithm is like refining a recipe. Each time you taste-test (E-step) and then tweak the ingredients (M-step), the blend becomes more representative of what customers want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def create_data_gmm():\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Means and covariances for three Gaussian distributions (coffee blends)\n",
    "    means = np.array([[2, 3], [8, 7], [5, 10]])  # sweetness and acidity means\n",
    "    covariances = [np.array([[1, 0.5], [0.5, 1]]),  # covariance matrix for blend 1\n",
    "                np.array([[1, -0.3], [-0.3, 1]]),  # covariance matrix for blend 2\n",
    "                np.array([[1, 0], [0, 1]])]  # covariance matrix for blend 3\n",
    "\n",
    "    # Number of points in each cluster (representing customers)\n",
    "    points_per_cluster = 100\n",
    "\n",
    "    # Generate points from each Gaussian distribution\n",
    "    X1 = np.random.multivariate_normal(means[0], covariances[0], points_per_cluster)\n",
    "    X2 = np.random.multivariate_normal(means[1], covariances[1], points_per_cluster)\n",
    "    X3 = np.random.multivariate_normal(means[2], covariances[2], points_per_cluster)\n",
    "\n",
    "    # Combine all points into one dataset\n",
    "    X = np.vstack((X1, X2, X3))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X = create_data_gmm()\n",
    "plt.scatter(X[:, 0], X[:, 1], s=30, color='b', label=\"Customers' coffee preferences\")\n",
    "plt.title('Synthetic Coffee Preferences Dataset')\n",
    "plt.xlabel('Sweetness')\n",
    "plt.ylabel('Acidity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### For the initialization of the parameters, we can do as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def initialize_params(X, n_clusters):\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    #Randomly initialize means from the data\n",
    "    means = X[np.random.choice(n_samples, n_clusters, False)]\n",
    "    \n",
    "    #initialize covariances as identity matrices\n",
    "    covariances = [np.eye(n_features) for _ in range(n_clusters)]\n",
    "    \n",
    "    #initialize equal weights for the mixture components\n",
    "    weights = np.ones(n_clusters) / n_clusters\n",
    "\n",
    "    return means, covariances, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 5a) First, complete the code which would help perform the `expectation step` of the EM algorithm. \n",
    "- You are required to fill in the correct code that would compute the `responsibility` (posterior probability that a point belongs to a cluster)\n",
    "- Remember that the `responsibility calculation uses the initialized parameters` (means, covariances, and weights). \n",
    "- Use the `multivariate_normal` function to `compute the pdf` of the multivariate Gaussian.\n",
    "- Return the computed `responsibilities` array. Please don't forget to normalize the responsibilities before returning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "def expectation_step(X, means, covariances, weights):\n",
    "    n_samples, n_clusters = X.shape[0], len(means)\n",
    "    responsibilities = np.zeros((n_samples, n_clusters))     #initialize the responsibilities as a np array of 0s\n",
    "    #compute the responsibilities by iterating over each cluster (for each k, compute responsibilities)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 5b) Secondly, complete the code which would help perform the `maximization step` of the EM algorithm. \n",
    "- In the below code, you are required to complete the code for `computing the covariances`.\n",
    "- As a refresher, the update formulas are given below.\n",
    "\n",
    "**Mean Update**\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} x_i}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "$$\n",
    "\n",
    "\n",
    "**Covariance Update**\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T}{\\sum_{i=1}^{N} \\gamma_{ik}}\n",
    "$$\n",
    "\n",
    "\n",
    "**Weights Update**\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik}}{N}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $\\gamma_{ik}$ is the responsibility of component $k$ for data point $i$.\n",
    "- $N$ is the total number of data points.\n",
    "- $x_i$ is the data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximization_step(X, responsibilities):\n",
    "    n_samples, n_clusters = responsibilities.shape\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    #initialize parameters\n",
    "    means = np.zeros((n_clusters, n_features))\n",
    "    covariances = []\n",
    "    weights = np.zeros(n_clusters)\n",
    "    \n",
    "    for k in range(n_clusters):\n",
    "        # effective number of points assigned to cluster k\n",
    "        Nk = responsibilities[:, k].sum()\n",
    "        \n",
    "        #update the means\n",
    "        means[k] = (X * responsibilities[:, k][:, np.newaxis]).sum(axis=0) / Nk\n",
    "        #update the covariance matrices\n",
    "        covariance_k = np.zeros((n_features, n_features)) \n",
    "\n",
    "         \n",
    "        for i in range(n_samples):\n",
    "            ...\n",
    "        covariances.append(covariance_k / Nk)\n",
    "        \n",
    "        #update the weights (mixture proportions)\n",
    "        weights[k] = Nk / n_samples\n",
    "    \n",
    "    return means, covariances, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 5c) Complete the code which would help compute the `log-likelihood` of the multivariate normal distribution using the `3 calculated parameters` (mean, covariance, and weights) from the EM steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Log-Likelihood Calculation:**\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^{N} \\log \\left( \\sum_{k=1}^{K} \\pi_k \\cdot \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $\\log L(\\theta)$ is the log-likelihood of the parameters $\\theta$.\n",
    "- $N$ is the total number of data points.\n",
    "- $K$ is the number of components.\n",
    "- $\\pi_k$ is the mixing coefficient for component $k$.\n",
    "- $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ is the Gaussian probability density function for data point $x_i$ given mean $\\mu_k$ and covariance $\\Sigma_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_likelihood(X, means, covariances, weights):\n",
    "    n_samples, n_clusters = X.shape[0], len(means)\n",
    "    log_likelihood = 0\n",
    "    ...\n",
    "    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 5d) Write a function `em_algorithm` which would:\n",
    "1. `Initialize` the parameters\n",
    "2. Iteratively perform the `expectation step`\n",
    "3. Perform the `maximization step`\n",
    "4. Compute the`Log-likelihood` of the multivariate Gaussian\n",
    "5. Check for `convergence `\n",
    "6. Return `means, covariances, weights, responsibilities, log_likelihoods`\n",
    "\n",
    "- You can do this by calling the appropriate functions that you completed from 5a) through 5c) until step 4.\n",
    "- For step 5, check if the last and penultimate log likelihoods have a difference below a threshold value `tol` and break if that condition is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def em_algorithm(X, n_iters=100, tol=1e-4):\n",
    "    log_likelihoods = [] # use this list to store all the log-likelihood values\n",
    "    n_clusters = 3\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5e) Run the below code to visualize your GMM clusters means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "means, covariances, weights, responsibilities, log_likelihoods = em_algorithm(X, n_clusters)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=30, color='b', label=\"Data points\")\n",
    "plt.scatter(means[:, 0], means[:, 1], s=100, color='r', label=\"Estimated Means\", marker='x')\n",
    "plt.title('Clusters Found by Gaussian Mixture Model')\n",
    "plt.xlabel('Sweetness')\n",
    "plt.ylabel('Acidity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. \n",
    "Please submit the completed notebook on Gradescope to view your results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q3a": {
     "name": "q3a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n_samples = 300\n>>> n_features = 2\n>>> n_clusters = 4\n>>> random_state = 42\n>>> X, _ = make_blobs(n_samples=n_samples, centers=n_clusters, n_features=n_features, random_state=random_state)\n>>> assert ans3a(X)[1] == 'ward'\n>>> assert ans3a(X)[2] == 4\n>>> assert 0.75 < ans3a(X)[3] < 0.85\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = create_data()\n>>> assert 0.15 <= evaluate_gmm(X)[2][-1] <= 0.25\n>>> assert 0.5 <= evaluate_gmm(X)[2][0] <= 0.6\n>>> assert 0.35 <= evaluate_gmm(X)[2][4] <= 0.45\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = create_data()\n>>> assert optimal_k(X) == 4\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = create_data_gmm()\n>>> means, covariances, weights = initialize_params(X, 3)\n>>> assert expectation_step(X, means, covariances, weights).shape == (300, 3)\n>>> assert 0 < float(expectation_step(X, means, covariances, weights)[0][2]) <= 0.1\n>>> assert 0.2 < float(expectation_step(X, means, covariances, weights)[-1][1]) <= 0.3\n>>> assert 0.7 < float(expectation_step(X, means, covariances, weights)[-1][0]) <= 0.8\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = create_data_gmm()\n>>> means, covariances, weights = initialize_params(X, 3)\n>>> assert len(maximization_step(X, expectation_step(X, means, covariances, weights))[1]) == 3\n>>> assert maximization_step(X, expectation_step(X, means, covariances, weights))[1][0].shape == (2, 2)\n>>> assert 1.3 <= float(maximization_step(X, expectation_step(X, means, covariances, weights))[1][0][0][0]) <= 1.4\n>>> assert 2.5 <= float(maximization_step(X, expectation_step(X, means, covariances, weights))[1][0][1][1]) <= 2.7\n>>> assert 3.5 <= float(maximization_step(X, expectation_step(X, means, covariances, weights))[1][1][0][0]) <= 3.6\n>>> assert float(maximization_step(X, expectation_step(X, means, covariances, weights))[1][2][0][1]) == float(maximization_step(X, expectation_step(X, means, covariances, weights))[1][2][1][0])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5c": {
     "name": "q5c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = create_data_gmm()\n>>> means, covariances, weights = initialize_params(X, 3)\n>>> assert -1300 <= float(log_likelihood(X, maximization_step(X, expectation_step(X, means, covariances, weights))[0], maximization_step(X, expectation_step(X, means, covariances, weights))[1], maximization_step(X, expectation_step(X, means, covariances, weights))[2])) <= -1200\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5d": {
     "name": "q5d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = create_data_gmm()\n>>> assert -1200 <= float(em_algorithm(X)[4][-1]) <= -1100\n>>> assert 0.3 <= float(em_algorithm(X)[2][0]) <= 0.4\n>>> assert 0.9 < float(em_algorithm(X)[3][-1][0]) <= 1.0\n>>> assert 4.5 <= float(em_algorithm(X)[0][0][0]) <= 5.5\n>>> assert 0 <= em_algorithm(X)[1][0][1][0] == em_algorithm(X)[1][0][0][1] <= 0.1\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
