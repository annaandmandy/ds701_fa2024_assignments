{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720bc3a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"assignment6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626eded",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors, Decision Trees, and Random Forests\n",
    "\n",
    "In this assignment, you'll explore Decision Trees and Random Forests using the sklearn library. Follow the instructions, run all the cells, and submit the completed notebook via Gradescope.\n",
    "\n",
    "For multiple choice questions, make sure to only include your final answer. For all code set `random_state=42`.\n",
    "\n",
    "Throughout the assignment, you'll learn how to:\n",
    "\n",
    "- Build a Decision Tree\n",
    "- Compute the Gini index\n",
    "- Construct a Random Forest and adjust its parameters to optimize performance\n",
    "\n",
    "Let's begin by loading the necessary libraries and dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d41b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a dataset (Iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Show dataset features and labels\n",
    "print(\"Features:\", data.feature_names)\n",
    "print(\"Labels:\", data.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422db09",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 1. What is the primary difference between classification and regression problems?\n",
    "\n",
    "        A) Classification predicts continuous values, and regression predicts categories.\n",
    "        B) Classification predicts categories, and regression predicts continuous values.\n",
    "        C) Both are used to predict categories.\n",
    "        D) Both are used to predict numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0706e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ans1():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360583bc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### What is the purpose of the \"splitting criterion\" in a decision tree?\n",
    "\n",
    "        A) To evaluate when to stop splitting\n",
    "        B) To determine the optimal depth of the tree\n",
    "        C) To select the best feature to split on\n",
    "        D) To calculate the number of leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc361893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ans2():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999582aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "### 3. Building a Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86258a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 3. Building a Decision Tree:\n",
    "Create a Decision Tree and make sure the accuracy is above 90%. Also print out the tree with export_text(). Make sure to set the `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f23c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# Build a decision tree classifier Set (random_state=42)\n",
    "...\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "...\n",
    "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display the tree structure\n",
    "...\n",
    "print(tree_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4fa08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e82b6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "#### 4. Implement Gini Index\n",
    "\n",
    "Let's manually compute the Gini index for one of the splits in the decision tree.\n",
    "\n",
    "Here is a description of Gini index from the [lecture notes](https://tools4ds.github.io/DS701-Course-Notes/14-Classification-I-Decision-Trees.html#impurity-measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94bea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate Gini Index\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        ...\n",
    "    return gini\n",
    "\n",
    "# Example: Manually splitting data based on feature 0 (sepal length) with threshold 5.5\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Combine X and y into a dataset\n",
    "dataset = np.column_stack((X_train, y_train))\n",
    "\n",
    "# Test split\n",
    "left_group, right_group = test_split(0, 5.5, dataset)\n",
    "\n",
    "# Calculate Gini index for the split\n",
    "gini = gini_index([left_group, right_group], np.unique(y_train))\n",
    "print(f\"Gini Index for the split: {gini:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a506a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351489b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 5. Prevent Overfitting.\n",
    "\n",
    "Overfitting happens when the model learns the training data too well, including the noise, which results in poor performance on unseen data. Random Forests can also overfit, especially if the trees are too deep or there are too many trees.\n",
    "\n",
    "To prevent overfitting, we can:\n",
    "- Limit the depth of each tree (`max_depth`).\n",
    "- Reduce the number of trees (`n_estimators`).\n",
    "- Increase the minimum number of samples required to split a node (`min_samples_split`).\n",
    "- Increase the minimum number of samples required to be at a leaf node (`min_samples_leaf`).\n",
    "- Use cross-validation to monitor the model's performance.\n",
    "\n",
    "Tweak the parameters of the Random Forest to prevent overfitting. The accuracy should be less than 95% & above 80%. Make sure you use `random_state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74243f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Place the right one\n",
    "# Set (random_state=42)\n",
    "...\n",
    "rf_clf_tuned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict and calculate accuracy for the tuned model\n",
    "y_pred_rf_tuned = rf_clf_tuned.predict(X_test)\n",
    "rf_accuracy_tuned = accuracy_score(y_test, y_pred_rf_tuned)\n",
    "print(f\"Tuned Random Forest Accuracy: {rf_accuracy_tuned:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514b6d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d987e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 6. Interpretability with Decision Trees \n",
    "\n",
    "The decision tree is easy to interpret. Write code through the export_graphviz that can create an image to explain the decision tree.\n",
    "\n",
    "You can refer to the given image provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e958ab8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# Read the image file and display it using matplotlib\n",
    "img = mpimg.imread(\"decision_tree.png\")\n",
    "plt.figure(figsize=(8, 17))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6adeea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "# Use Tree CLF to export the tree structure to a dot file\n",
    "...\n",
    "\n",
    "# Display the tree using graphviz\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "# Save the graph to a file\n",
    "graph.format = 'png'\n",
    "graph.render(\"figs/decision_tree\")\n",
    "\n",
    "# Read the image file and display it using matplotlib\n",
    "img = mpimg.imread(\"figs/decision_tree.png\")\n",
    "plt.figure(figsize=(8, 17))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc90cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7dabb6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### 7. Interpretability with kNN? \n",
    "\n",
    "It can be somewhat difficult to __understand__ why $k$-NN is making a specific prediction. It depends on the data in the neighborhood of the test point.\n",
    "\n",
    "Let us build a K-NN. Do the following:\n",
    "- Scale the features\n",
    "- Build the k-NN with 5 neighbors\n",
    "- Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed355bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Standardize the features, X_train_scaled, and X_test_scaled\n",
    "...\n",
    "\n",
    "\n",
    "# Create the KNN model Set (random_state=42)\n",
    "...\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "...\n",
    "plt.title(f'Confusion Matrix for k-NN (Accuracy: {accuracy:.2f})')\n",
    "plt.show()\n",
    "\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26300b7c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67994a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753d51a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53135d68",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q3": {
     "name": "q3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(tree_clf, DecisionTreeClassifier), 'tree_clf is not an instance of DecisionTreeClassifier'\n>>> assert hasattr(tree_clf, 'tree_'), 'DecisionTreeClassifier has not been fitted yet'\n>>> assert accuracy > 0.9, 'The accuracy of the Decision Tree Classifier is too low'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (left_group, right_group) = test_split(0, 6.5, dataset)\n>>> gini = gini_index([left_group, right_group], np.unique(y_train))\n>>> assert type(gini) == float\n>>> assert np.round(gini, 3) == 0.584\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> output = rf_clf_tuned.fit(X_train, y_train)\n>>> y_pred_rf = rf_clf_tuned.predict(X_test)\n>>> rf_accuracy = accuracy_score(y_test, y_pred_rf)\n>>> assert type(rf_accuracy) == float\n>>> assert np.round(rf_accuracy, 2) <= 0.95\n>>> assert np.round(rf_accuracy, 2) >= 0.8\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import os\n>>> files_in_figs = os.listdir('figs')\n>>> assert 'decision_tree.png' in files_in_figs, \"'decision_tree.png' is not in the 'figs' folder.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert knn.n_neighbors == 5\n>>> assert X_train_scaled.shape == X_train.shape, 'X_train_scaled has different shape than X_train.'\n>>> assert X_test_scaled.shape == X_test.shape, 'X_test_scaled has different shape than X_test.'\n>>> assert X_train is not X_train_scaled, 'X_train and X_train_scaled refer to the same object.'\n>>> assert not np.array_equal(X_train, X_train_scaled), 'X_train and X_train_scaled have the same content.'\n>>> assert X_test is not X_test_scaled, 'X_test and X_test_scaled refer to the same object.'\n>>> assert not np.array_equal(X_test_scaled, X_test), 'X_train and X_train_scaled have the same content.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
